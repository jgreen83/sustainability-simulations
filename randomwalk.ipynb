{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is a high performance compiler that translates functions to optimized machine code. This allows us to run numerical algorithms on a GPU without learning C or FORTRAN. Designed  to be used with NumPy for scientific computing.\n",
    "\n",
    "The CUDA library from Numba supports GPU programming for a restricted subset of Python code in CUDA kernels allowing for parallelization of numerical algorithms. CUDA cores are available on Nvidia GPUs. Documentation can be found [here](https://numba.readthedocs.io/en/stable/cuda/overview.html).\n",
    "\n",
    "Numba's GPU random number generator is an implementation of the xoroshiro128+ algorithm. Other random number generators are not supported by cuda.\n",
    "\n",
    "The NumPy package provides a multidimensional array object along with functions providing functionality similar to MATLAB.\n",
    "\n",
    "Matplot is a plotting library used with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_normal_float32\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detect function from the cuda library will print a summary of supported CUDA hardware. This hardware must be present to use the CUDA framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 4050 Laptop GPU'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.9\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-4093b37a-da12-a30d-cc27-f706fbbec7ec\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 64\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function computes the splitting probability of a diffusing particle on the interval [0, 1] leaving the interval at x = 0 conditioned on starting at 1/2. It is implemented in using NumPy functions and will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_probability_cpu():\n",
    "    x = 1/2\n",
    "    while x > 0 and x < 1:\n",
    "        x += 0.01 * np.random.normal()\n",
    "    if x < 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare computation speed of CPU and GPU implementation we need to use CUDA's two-level hierarchy when declaring the number of threads. To do so we declare the number of blocks and the number of threads per block. The product of the two will give the total number of threads.\n",
    "\n",
    "Each device has an optimal threads per block. Benchmark testing revealed mine to be 512. This is typically a multiple of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = int(512)\n",
    "blocks = int(10 ** 5 / threads_per_block)\n",
    "trials = threads_per_block * blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following CUDA kernel runs the same simulation on the GPU. This looks similar to the CPU implementation with a few important differences. The \"@cuda.jit\" decorator which marks the function for \"just in time\" compiling to machine language. The rng_states input is required for use with the xoroshiro128+ RNG. \"thread_id\" tells the GPU to assign each thread to a different position in the output array. Additional options exist for more complicated computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def splitting_probability_gpu(rng_states, out):\n",
    "    thread_id = cuda.grid(1)\n",
    "    x=1/2\n",
    "    while x<1 and x>0:\n",
    "        x += 0.01 * xoroshiro128p_normal_float32(rng_states, thread_id)\n",
    "        if x<0:\n",
    "            out[thread_id] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a benchmark test, we run both implementations for the same number of trials, saving outputs and total run times. Note that the invocation of the CUDA kernel designates the total number of threads using \"[blocks, threads_per_block]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "#t0 = time.time()\n",
    "#out_cpu = np.zeros(trials, dtype=np.float32)\n",
    "#for i in range(trials):\n",
    "#    out_cpu[i] = splitting_probability_cpu()\n",
    "#run_time_cpu = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=np.random.randint(1, high = 1000))\n",
    "out_gpu = np.zeros(threads_per_block * blocks, dtype=np.float32)\n",
    "splitting_probability_gpu[blocks, threads_per_block](rng_states, out_gpu)\n",
    "run_time_gpu = time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the results of the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials:  99840\n",
      "CPU splitting probability:  0.5021587\n",
      "GPU splitting probability:  0.5021234\n",
      "CPU total computation time:  29.281405687332153\n",
      "GPU total computation time:  0.23433232307434082\n",
      "GPU speed increase:  124.95675075112351\n"
     ]
    }
   ],
   "source": [
    "mean_cpu = np.mean(out_cpu)\n",
    "mean_gpu = np.mean(out_gpu)\n",
    "\n",
    "print(\"Number of trials: \", str(trials))\n",
    "print(\"CPU splitting probability: \", str(mean_cpu))\n",
    "print(\"GPU splitting probability: \", str(mean_gpu))\n",
    "print(\"CPU total computation time: \", str(run_time_cpu))\n",
    "print(\"GPU total computation time: \", str(run_time_gpu))\n",
    "print(\"GPU speed increase: \", str(run_time_cpu / run_time_gpu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
